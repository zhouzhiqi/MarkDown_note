# 线性回归模型

## 简介

* 给定训练数据$D=\{\mathbf{X}_i , y_i\}_{i=1}^{N}$ , 其中$y\in \mathbb{R}$, 
* 回归学习一个从输入$\mathbf{x}$到输出$y$的映射$f$
* 对新的测试数据$\mathbf{x}$, 用学习到的映射$f$对其进行预测:$\hat{y} = f(\mathbf{x})$
* 若假设映射$f$是一个线性函数, 即$y=f(\mathbf{x} | \mathbf{w}) = \mathbf{w}^T \mathbf{x}$
* 我们称之为线性回归模型

## 目标函数

* 目标函数通常包含两项: **损失函数**和**正则项**

$$
J(\mathbf{\theta})=\sum_{i=1}^{N} L(f(\mathbf{x}_i , \mathbf{\theta}), y_i) +\lambda\Omega(\mathbf{\theta})
$$

### 损失函数

- 可以采用$L2$损失得到, 残差平方和(residual sum  of squares, RSS)

$$
\begin{equation}
\begin{aligned}
J(\mathbf{\theta}) & =\sum_{i=1}^{N} L( y_i , \hat{y_i} ) 
= \sum_{i=1}^{N} ( y_i - \hat{y_i} ) ^2 \\ 
& =\sum_{i=1}^{N} ( y_i -\mathbf{w}^T \mathbf{x}_i )^2
\end{aligned}
\end{equation}
$$



### 正则项

* 为 **0** 时, 得到最小二乘线性回归(Ordinary Least Square, OLS)

$$
\begin{equation}
\begin{aligned}
J(\mathbf{w}) & =\sum_{i=1}^{N} L( y_i , \hat{y_i} ) 
= \sum_{i=1}^{N} ( y_i - \hat{y_i} ) ^2 \\ 
& =\sum_{i=1}^{N} ( y_i -\mathbf{w}^T \mathbf{x}_i )^2
\end{aligned}
\end{equation}
$$

* 为$L1$正则时,得到Lasso模型: 当$\lambda$取合适值时, Lasso的结果稀疏的, 可以起到特征选择的作用

$$
J(\mathbf{w})  =\sum_{i=1}^{N} ( y_i -\mathbf{w}^T \mathbf{x}_i )^2
+\lambda | \mathbf{w} |
$$

* 为$L2$正则时,得到岭回归(Ridge Regression)模型: 

$$
J(\mathbf{w})  =\sum_{i=1}^{N} ( y_i -\mathbf{w}^T \mathbf{x}_i )^2
+\lambda \| \mathbf{w} \| _2^2
$$



## 概率解释

- 无正则, 最小二乘回归等价于极大似然估计
- $L1$正则, Lasso模型等价于Laplace先验下的贝叶斯估计
- $L2$正则, 岭回归模型等价于高斯先验下的贝叶斯估计

### 极大似然估计

* 假设: $y = f( \mathbf{x} ) + \varepsilon = \mathbf{w}^T \mathbf{x} + \varepsilon$, 其中$\varepsilon$为线性预测和真值之间的残差
* 假设残差$\varepsilon $的分布服从均值为$0$, 方差为$\sigma^2$的正态分布: $\varepsilon \sim  \mathcal{N} (0, \sigma^2)$, 因此线性回归$y$服从正态分布为$f(\mathbf{x}) + \varepsilon \sim  \mathcal{N} (f(\mathbf{x});  \sigma^2)$, 也即均值加上$\mathbf{w}^T \mathbf{x}$ : 

$$
p (y | \mathbf{x,\theta}) \sim  \mathcal{N} (y | \mathbf{w}^T \mathbf{x}, \sigma^2), \theta =( \mathbf{w}; \sigma^2)
$$

* **似然函数**(log)表示: 在参数为$\theta​$ 的情况下, 数据$D=\{\mathbf{x}_i , y_i\}_{i=1}^{N}​$出现的概率

$$
l (\theta) = log \ p(D | \theta) = \sum_{i=1}^N log \ p(y_i |\mathbf{x}_i; \theta)
$$

- **极大似然估计**(Maximize Likelihood Estimator, MLE)表示为: 所选择的数据$D=\{\mathbf{x}_i , y_i\}_{i=1}^{N}​$以最大概率出现时对应的$\theta​$的取值

$$
\hat{\theta} = arg_\theta \ max \ log \ p(D | \theta)
$$

* **线性回归的MLE**

$$
\begin{equation}
\begin{aligned}
p (y_i | \mathbf{x}_i, \mathbf{w}, \sigma^2) &=  \mathcal{N} (y_i | \mathbf{w}^T \mathbf{x}_i; \sigma^2) \\
&=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left( -\frac{1}{2 \sigma^2}(y_i - \mathbf{w}^T \mathbf{x}_i)^2 \right)
\end{aligned}
\end{equation}
$$

### MLE_OLS

* 似然函数为:

$$
l (\theta) = log \ p(D | \theta) 
= \sum_{i=1}^N log \ p(y_i |\mathbf{x}_i; \theta)
$$

* 极大似然可等价地写成**极小负log似然损失**(negative log
  likelihood, NLL)

$$
\begin{equation}
\begin{aligned}
NLL (\mathbf{\theta}) &= -\sum_{i=1}^N log \ p(y_i |\mathbf{x}_i; \mathbf{w}, \sigma^2) \\
&=-\sum_{i=1}^N \ log \ 
\begin{bmatrix}
\left( \frac{1}{2 \pi \sigma^2} \right) ^{\frac{1}{2}} \exp \left( -\frac{1}{2 \sigma^2}(y_i - \mathbf{w}^T \mathbf{x}_i)^2 \right) 
\end{bmatrix}\\
&=- \sum_{i=1}^N\begin{bmatrix} 
\left( -\frac{1}{2}log(2 \pi \sigma^2) \right)
+\left( -\frac{1}{2\sigma^2}(y_i - \mathbf{w}^T \mathbf{x}_i)^2\right)
\end{bmatrix}\\
&=\frac{N}{2}log(2 \pi \sigma^2) + \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - \mathbf{w}^T \mathbf{x}_i)^2
\end{aligned}
\end{equation}
$$

### MLE_$L2$正则回归
$$
p (y | \mathbf{x,\theta}) \sim  \mathcal{N} (y | \mathbf{w}^T \mathbf{x}; \sigma^2), \theta =( \mathbf{w}, \sigma^2)
$$

$$
p (\mathbf{y} | \mathbf{X; w, \sigma^2}) 
=  \mathcal{N} (y | \mathbf{Xw, \sigma^2 I}_N) \\
\propto \exp \left( -\frac{1}{2 \sigma^2}(\mathbf{y-Xw})^T(\mathbf{y-Xw}) \right)
$$

$$
若假设参数w的先验分布为高斯分布\\
w_i \sim \mathcal{N}(0,\tau^2), 
其中\frac{1}{\tau^2}控制先验的强度
$$

$$
p(\mathbf{w})=\prod_{j=1}^D \mathcal{N}(w_j | 0, \tau^2) \\
\propto \exp \left( -\frac{1}{2\tau^2} \sum_{j=1}^D \mathbf{w}_j^2 \right)
=\exp \left( -\frac{1}{2\tau^2}[\mathbf{w}^T \mathbf{w}] \right)
$$

$$
根据贝叶斯公式, 得到参数的后验分布为\\
p (\mathbf{w} | \mathbf{X, y, \sigma^2}) \\ 
\propto 
	\exp \left( -\frac{1}{2 \sigma^2}
	\begin{bmatrix}(\mathbf{y-Xw})^T(\mathbf{y-Xw})\end{bmatrix}
	-\frac{1}{2\tau^2}
	\begin{bmatrix}\mathbf{w}^T \mathbf{w}\end{bmatrix} \right)
$$

$$
最大化后验估计等价于最小化目标函数\\
J(\mathbf{w})=[(\mathbf{y-Xw})^T(\mathbf{y-Xw}) ]
+ \frac{\sigma^2}{\tau^2}[\mathbf{w}^T \mathbf{w}]
$$

### MLE_$L1$正则回归

$$
p (y | \mathbf{x,\theta}) \sim  \mathcal{N} (y | \mathbf{w}^T \mathbf{x}, \sigma^2), \theta =( \mathbf{w}, \sigma^2)
$$

$$
p (\mathbf{y} | \mathbf{X, w, \sigma^2}) 
=  \mathcal{N} (y | \mathbf{Xw, \sigma^2 I}_N) \\
\propto \exp \left( -\frac{1}{2 \sigma^2}(\mathbf{y-Xw})^T(\mathbf{y-Xw}) \right)
$$

$$
若假设参数w的先验分布为Laplace分布\\
w_i \sim \mathcal{Laplace}(0,\frac{1}{\tau}), 
其中{\tau}控制先验的强度
$$

$$
p(\mathbf{w})=\prod_{j=1}^D \mathcal{Laplace}(w_j | 0, \frac{1}{\tau}) \\
\propto \exp \left( -\lambda \sum_{j=1}^D | \mathbf{w}_j | \right)
$$

$$
根据贝叶斯公式, 得到参数的后验分布为\\
p (\mathbf{w} | \mathbf{X, y, \sigma^2}) \\ 
\propto 
	\exp \left( -\frac{1}{2 \sigma^2}
	\begin{bmatrix}(\mathbf{y-Xw})^T(\mathbf{y-Xw})\end{bmatrix}
	 -\lambda \sum_{j=1}^D | \mathbf{w}_j | \right)
$$

$$
最大化后验估计等价于最小化目标函数\\
J(\mathbf{w})=[(\mathbf{y-Xw})^T(\mathbf{y-Xw}) ]
+ \frac{2\sigma^2}{\tau} | \mathbf{w} |
$$

## 优化求解

* 通过训练数据, 得到最优的参数$w$, 即求目标函数取极小值时的参数$w$:

$$
\hat{\mathbf{W}} = arg_\mathbf{W} \ min \ J(\mathbf{W})
$$

* 根据模型特点和问题复杂度, 可采用不同优化算法:

$$
一阶导=0: \frac{\partial J(\mathbf{W})}{\partial \mathbf{W}}=0\\
二阶导>0: \frac{\partial J^2(\mathbf{W})}{\partial \mathbf{W}}>0
$$

### OLS_矩阵伪逆法

* 将OLS的目标函数写成矩阵形式(为求目标函数取极小值时的$w$, 因子$\frac{1}{N}$不影响$w$, 则省略不写)

$$
\begin{equation}
\begin{aligned}
J(\mathbf{w}) & =\sum_{i=1}^{N} ( y_i -\mathbf{w}^T \mathbf{x}_i )^2
= (\mathbf{y-Xw})^T(\mathbf{y-Xw}) \\
& =\left( \mathbf{y^Ty-2w^T(X^Ty)+w^T(X^TX)w}\right)
\end{aligned}
\end{equation}
$$

* 由于未知数只有$\mathbf{w}$, 只取与$\mathbf{w}$ 有关的项得

$$
J(\mathbf{w}) = \mathbf{-2w^T(X^Ty)+w^T(X^TX)w}
$$

* 对$\mathbf{w}$ 求一阶导得

$$
\frac{\partial J(\mathbf{w})}{\partial\mathbf{w}}  
= \mathbf{-2(X^Ty)+2(X^TX)w} = 0
$$

$$
\to \mathbf{2(X^TX)w=2X^Ty}\\
\to \mathbf{(X^TX)w=X^Ty}
$$

$$
\Rightarrow \mathbf{\hat{w}}_{OLS}=\mathbf{(X^TX)^{-1}X^Ty}
$$

$$
\mathbf{(X^TX)}是对称矩阵,\mathbf{(X^TX)^{-1}}为\mathbf{(X^TX)}的逆或伪逆
$$

* 伪逆的求法: 通常对输入$\mathbf{X}$ 进行**奇异值分解** (singular value decomposition҅, SVD)

$$
\mathbf{X=U \Sigma V^T} \ \Leftrightarrow \ \mathbf{X^T=V \Sigma U^T}
$$

$$
\Rightarrow \mathbf{X^TX=V \Sigma U^T U \Sigma V^T = \Sigma^2}
$$

$$
(U, V均为正交矩阵, 即U^T=U^{-1}, 则U^TU=1)
$$

### 梯度下降法

* 在求解机器学习算法的模型参数(无约束优化问题)时, **梯度下降**(Gradient Descent) 是最常用的方法之一. 又称**最速下降法**
* 主要应用于一阶导数最优化算法
* 一元函数$f(x)$ 在$x$ 处的梯度为函数$f$ 在点$x$ 处的导数
* 多元函数$f(x_1, x_2, ..., x_D)$ 在点$\mathbf{x} = (x_1, x_2, ..., x_D)$ 处共有$D$ 个偏导数, 分别是$f()$ 在该点关于$x_i$ 的偏导数组成的一个$D$ 维的向量, 称这个$D$维向量为$f$ 在$\mathbf{x}$ 处的梯度